# -*- coding: utf-8 -*-
"""Yaniv_Kaveh_Shtul_206768004.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qwAm_y3p5D1imcHvP_S2jsXYOFrClCMS

# * **Name**: Yaniv Kaveh Shtul
# * **Id**: 206768004
# * **Profile_url**: https://www.kaggle.com/yanivkavehshtul

# **The competition**

**The sinking of the Titanic is one of the most infamous shipwrecks in history.
On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.
While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.
In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).**


**I will try to build the model by analyzing, engineering and experimenting on the data.**

# **Import libraries**
"""

# import numpy, matplotlib, etc.
import math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

# sklearn imports
import sklearn
from sklearn import metrics
from sklearn import datasets
from sklearn import pipeline
from sklearn import linear_model
from sklearn import preprocessing
from sklearn import model_selection
from sklearn.preprocessing import OneHotEncoder
from sklearn import neural_network

# define plt settings
sns.set_theme()
plt.rcParams["font.size"] = 20
plt.rcParams["axes.labelsize"] = 20
plt.rcParams["xtick.labelsize"] = 20
plt.rcParams["ytick.labelsize"] = 20
plt.rcParams["legend.fontsize"] = 20
plt.rcParams["legend.markerscale"] = 1.5
plt.rcParams["figure.figsize"] = (12, 10)
plt.rcParams["legend.title_fontsize"] = 20

"""# **Importing data**"""

titanic_df = pd.read_csv("/kaggle/input/titanic/train.csv")
titanic_test = pd.read_csv("/kaggle/input/titanic/test.csv")
submission = pd.read_csv("/kaggle/input/titanic/gender_submission.csv")
titanic_df

"""# Dataset info"""

titanic_df.info()

"""# **Remove unnecessary columns**

**The "Name" column is not important. The important data of the column is the family name to indentify other family members on the ship.
This data is in the columns: "SibSp" and "Parch", thats why removing the column will not make an impact on the result.
"PassengerId" column is not important because it is just a counter of passengers, thats why im dropped the coulmn too.
I dropped "Ticket" column because it is just a "random" number.**
"""

titanic_df = titanic_df.drop(columns=['Name', 'PassengerId', 'Ticket'], axis=1)
titanic_df

#Changing the "Sex" column to numeric, male - 1.0 , female - 0.0
#titanic_df = pd.get_dummies(titanic_df,dtype=float, columns=['Sex'], prefix=["Sex_type_is"], drop_first=True)
#titanic_df

"""# **Insert values into null cells**"""

#Dataset info
titanic_df.info()

# replace all empty values to np.NaN values
titanic_df.replace('', np.NaN, inplace=True)
titanic_df.fillna(np.NaN, inplace=True)
titanic_df

# count empty values in each column
def count_empty_values_in_each_column(df):
    print('empty values:')
    code = "len(np.where(df[column].isnull())[0])"
    for column in df.columns:
        print(f'`{column}`: {eval(code)}')

count_empty_values_in_each_column(titanic_df)

"""**As we can see there are empty cells in the features: Age, Cabin and Embarked. 77% of the Cabin feature missing values and filling this many cells can impact on the resualt. We will insert values and I will train the dataset with and without the feature to find the better result**"""

# fill empty values in the dataframe
def fill_na_median(df, column_name):
    df_not_null = df[~df[column_name].isnull()]
    df[column_name].fillna(df_not_null[column_name].median(), inplace=True)

def fill_na_random_pick_column_distribution(df, column_name):
    df_not_null = df[~df[column_name].isnull()]
    df_null = df[df[column_name].isnull()]
    options = np.random.choice(df_not_null[column_name])
    df[column_name] = df[column_name].apply(lambda x: np.random.choice(df_not_null[column_name]) if pd.isnull(x) else x)

fill_na_median(titanic_df,'Age')
fill_na_random_pick_column_distribution(titanic_df, 'Cabin')
fill_na_random_pick_column_distribution(titanic_df, 'Embarked')
titanic_df

# check for empty values
count_empty_values_in_each_column(titanic_df)

"""# Encode columns"""

# using LabelEncoder from sklearn to change Cabin to numbers depanding their lables
lb = sklearn.preprocessing.LabelEncoder()
titanic_df['Cabin'] = lb.fit_transform(titanic_df['Cabin'])

# dummy encode Sex and Embarked
titanic_df = pd.get_dummies(titanic_df, columns=['Sex', 'Embarked'], prefix=["Sex_type_is", "Embarked at"], drop_first=True, dtype=float)
titanic_df

"""# Test data manipulation (same as train_set)"""

#Cleaning data
titanic_test = titanic_test.drop(columns=['Name', 'PassengerId', 'Ticket'], axis=1)
titanic_test

titanic_test.info()

# replace all empty values to np.NaN values
titanic_test.replace('', np.NaN, inplace=True)
titanic_test.fillna(np.NaN, inplace=True)

# count empty values in each column
count_empty_values_in_each_column(titanic_test)
titanic_test

#Filling empty cells
fill_na_median(titanic_test,'Age')
fill_na_median(titanic_test,'Fare')

fill_na_random_pick_column_distribution(titanic_test, 'Cabin')
fill_na_random_pick_column_distribution(titanic_test, 'Embarked')
titanic_test

# check for empty values
count_empty_values_in_each_column(titanic_test)

# using LabelEncoder from sklearn to change Cabin to numbers depanding their lables
lb = sklearn.preprocessing.LabelEncoder()
titanic_test['Cabin'] = lb.fit_transform(titanic_test['Cabin'])

# dummy encode Sex and Embarked
titanic_test = pd.get_dummies(titanic_test, columns=['Sex', 'Embarked'], prefix=["Sex_type_is", "Embarked at"], drop_first=True, dtype=float)
titanic_test

"""# Features engineering"""

# get color map
def get_sns_cmap(n, name='muted'):
    return sns.color_palette(palette=name, n_colors=n)

# plot with regression line target values by each feature
def plot_reg_target_values_by_each_feature(df, target_column_name):
    nrows = math.ceil(math.sqrt(len(df.columns)-1))
    ncols = math.ceil((len(df.columns)-1)/nrows)
    fig, axes = plt.subplots(nrows, ncols)
    plt.subplots_adjust(top=3.2, bottom=0, left=0, right=2.5)
    colors = get_sns_cmap(len(df.columns))

    counter = 0
    for i in range(len(df.columns)-1):
        ax = sns.regplot(x=df.columns[i], y=target_column_name, data=df, color=colors[i], ax=axes[i//nrows, i%nrows], scatter_kws={"s": 20})
        ax.set_title(label=f'{df.columns[i]} by {target_column_name}', fontsize=10)

    for i in range(len(df.columns)-1, nrows*ncols):
        fig.delaxes(axes.flatten()[i])


plot_reg_target_values_by_each_feature(titanic_df, 'Survived')

"""**It is hard to decide with feature as a good correlation. Lets check the heatmap for more information.**"""

# show absolute correlation between features in a heatmap
plt.figure(figsize=(18,10))
cor = np.abs(titanic_df.corr())
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, vmin=0, vmax=1)
plt.show()

"""**It is look like that "Sex_type_is_male" has a good correlation with survived. In addition "Fare" and "Pclass" has good correlation. Lets try to make a new features that will connect both.**

**The new feature is: Fare_Class = Fare / Pclass**
"""

# Adding the new feature to the dataset
titanic_df.insert(len(titanic_df.columns), 'Fare_Class', titanic_df['Fare'] / titanic_df['Pclass'])
titanic_df

"""**Lets check if the new feature in a good correlation with the "Survived"(goal)**"""

# show absolute correlation between features in a heatmap
plt.figure(figsize=(18,10))
cor = np.abs(titanic_df.corr())
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, vmin=0, vmax=1)
plt.show()

"""****As we can see the new feature as a good correlation with the goal so we will keep the feature and change the test as well. In addition the "Cabin" feature has bad correlation with the "Survived" and because of that we will save a copy of the titanic_df and titanic_test without the feature ****"""

# Adding the new feature to the Test
titanic_test.insert(len(titanic_test.columns), 'Fare_Class', titanic_test['Fare'] / titanic_test['Pclass'])
titanic_test

#make copies of titanic_df and titanic_test
n_cabin_df = titanic_df.copy()
n_cabin_test = titanic_test.copy()

#Dropping Cabin
n_cabin_df = n_cabin_df.drop('Cabin', axis=1)
n_cabin_test = n_cabin_test.drop('Cabin', axis=1)

n_cabin_df

"""# **Data Slicing**"""

# print 4 graphs: mse of train/test and r2 of train/test
def print_graphs_r2_mse(graph_points):
    for k, v in graph_points.items():
        best_value = max(v.values()) if 'R2' in k else min(v.values())
        best_index = np.argmax(list(v.values())) if 'R2' in k else np.argmin(list(v.values()))
        color = 'red' if 'train' in k else 'blue'
        fig = px.scatter(x=v.keys(), y=v.values(), title=f'{k}, best value: x={best_index + 1}, y={best_value}', color_discrete_sequence=[color])
        fig.data[0].update(mode='markers+lines')
        fig.show()

def plot_score_and_loss_by_split_SGD(x, t):
    graph_points = {
                    'train_CE':{},
                    'val_CE': {},
                    'train_R2': {},
                    'val_R2': {}
                    }

    for size in range(10, 100, 10):
        x_train, x_val, t_train, t_val = model_selection.train_test_split(x, t, test_size=size/100, random_state=42)
        SGD_cls = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log_loss', alpha=0, learning_rate='constant', eta0=0.01)).fit(x_train, t_train)
        y_train_prob = SGD_cls.predict_proba(x_train)
        y_test_prob = SGD_cls.predict_proba(x_val)
        y_train = SGD_cls.predict(x_train)
        y_val = SGD_cls.predict(x_val)
        graph_points['train_CE'][size/100] = metrics.log_loss(t_train, y_train_prob)
        graph_points['val_CE'][size/100] = metrics.log_loss(t_val, y_test_prob)
        graph_points['train_R2'][size/100] = SGD_cls.score(x_train, t_train)
        graph_points['val_R2'][size/100] = SGD_cls.score(x_val, t_val)
    print_graphs_r2_mse(graph_points)

"""# SGD train"""

# divide the data to features and target
t = titanic_df['Survived'].copy()
X = titanic_df.drop(['Survived'], axis=1)
print('t')
display(t)
print()
print('X')
display(X)

plot_score_and_loss_by_split_SGD(X, t)

"""# Lets try with MLP train"""

def plot_score_and_loss_by_split_MLP(x, t):
        graph_points = {
                    'train_CE':{},
                    'val_CE': {},
                    'train_R2': {},
                    'val_R2': {}
                    }

        for size in range(10, 100, 10):
            x_train, x_val, t_train, t_val = model_selection.train_test_split(x, t, test_size=size/100, random_state=42)
            MLP_cls = neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=40000).fit(x_train, t_train)
            y_train_prob = MLP_cls.predict_proba(x_train)
            y_test_prob = MLP_cls.predict_proba(x_val)
            graph_points['train_CE'][size/100] = metrics.log_loss(t_train, y_train_prob)
            graph_points['val_CE'][size/100] = metrics.log_loss(t_val, y_test_prob)
            graph_points['train_R2'][size/100] = MLP_cls.score(x_train, t_train)
            graph_points['val_R2'][size/100] = MLP_cls.score(x_val, t_val)
        print_graphs_r2_mse(graph_points)

plot_score_and_loss_by_split_MLP(X, t)

"""# No Cabin SGD train"""

# divide the data to features and target
n_cabin_t = n_cabin_df['Survived'].copy()
n_cabin_X = n_cabin_df.drop(['Survived'], axis=1)
print('t')
display(n_cabin_t)
print()
print('X')
display(n_cabin_X)

plot_score_and_loss_by_split_SGD(n_cabin_X, n_cabin_t)

"""# Conclusion from the graph

**As we can see the SGD train has better result than the MLP train. thats why we will use the SGD train. Surprisingly the Cabin feature is necessary to get a better score, thats why we will use the train with the cabin feature.**

**In addition,We can see that from 0.1 to 0.7, the validation loss is smaller than the train loss, and from 0.75 to 0.9 the train loss is smaller than the validation loss.
So, let's give the validation group 25% of the dataset, it is about the right point where the validation loss is equale to the train loss.**

# Classification
"""

# split the data to 75% train and 25% validation
t = titanic_df['Survived']
X = titanic_df.drop('Survived', axis=1)
X_train, X_test, t_train, t_test = sklearn.model_selection.train_test_split(
    X, t, test_size=0.25, random_state=42)

# create the SGDClassifier and predict the probabilities of the train and test data
SGD_cls = pipeline.make_pipeline(
    preprocessing.StandardScaler(),
     linear_model.SGDClassifier(loss='log_loss', alpha=0,
                                learning_rate='constant',
                                eta0=0.01)
     ).fit(X_train, t_train)

y_train_prob = SGD_cls.predict_proba(X_train)
y_test_prob = SGD_cls.predict_proba(X_test)

y_train = SGD_cls.predict(X_train)
y_test = SGD_cls.predict(X_test)

# print the accuracy score and CE loss of the train and test
print('Accuracy score on train',
      SGD_cls.score(X_train, t_train))
print('Accuracy score on test',
      SGD_cls.score(X_test, t_test))
print()
print('CE on train',
      metrics.log_loss(t_train, y_train_prob))
print('CE on test',
      metrics.log_loss(t_test, y_test_prob))

"""# Predict test"""

prediction = SGD_cls.predict(titanic_test)

"""# Sumbission"""

submission['Survived'] = prediction
submission.to_csv('submission.csv', index=False)
submission

"""# Screenshots

![image.png](attachment:6c956130-6864-459b-84fd-a28f70603178.png)

![image.png](attachment:1239e777-ce97-4e72-bbdf-64f37af6c807.png)

# Conclusion

**After analyzing the data I eliminated unnecessary features and completed empty cells with median and random methods. I notice that 77% of the "Cabin" feature (in the training data) was empty that can impact negatively on the score. Surprisingly after checking the training data with and without the Cabin feature I found out that the feature is necessary to get a better score. With heat map I found out that "Fare" and "Pclass" have a good correlation so I made a feature out of them to increase the final score. Later I ran experiments on two different models: SGD and MLP. It turns out that the better model with the better score is SGD. In the end I spit the train into train set and validation set with ratio of 75% : 25% and ran the SGD model on the sets and the test to get the result.**

# References

sklearn site - https://scikit-learn.org/stable/

Afeka practice notebooks 2-4
"""